When we first talked about functors, we saw that they were a useful concept for values that can be
mapped over. Then, we took that concept one step further by introducing applicative functors,
which allow us to view values of certain data types as values with contexts and use normal functions
on those values while preserving the meaning of those contexts.


In this chapter, we'll learn about monads, which are just beefed up applicative functors, much like
applicative functors are only beefed up functors.


To answer this question of how to map a function over some data type, all we had to do was look at
the type of fmap:

fmap :: (Functor f) => (a -> b) -> f a -> f b

And then make it work for our data type by writing the appropriate Functor instance.

Then we saw a possible improvement of functors and said, hey, what if that function a -> b is
already wrapped inside a functor value? Like, what if we have Just (*3), how do we apply that to
Just 5? For this, the Applicative type class was introduced, in which we wanted the answer to the
following type:

(<*>) :: (Applicative f) => f (a -> b) -> f a -> f b

We also saw that we can take a normal value and wrap it inside a data type. For instance, we can
take a 1 and wrap it so that it becomes a Just 1. Or we can make it into a [1]. Or an I/O action
that does nothing and just yields 1. The function that does this is called pure.

Like we said, an applicative value can be seen as a value with an added context. For instance, the
character 'a' is just a normal character, whereas Just 'a' has some added context. Instead of a
Char, we have a Maybe Char, which tells us that its value might be a character, but it could also
be an absence of a character.

It was neat to see how the Applicative type class allowed us to use normal functions on these values
with context and how that context was preserved. Observe:

  ghci> (*) <$> Just 2 <*> Just 8
  => Just 16
  ghci> (++) <$> Just "klingon" <*> Nothing
  => Nothing
  ghci> (-) <$> [3,4] <*> [1,2,3]
  => [2,1,0,3,2,1]


Ah, cool, so now that we treat them as applicative values, Maybe a values represent computations
that might have failed, [a] values represent computations that have several results
(non-deterministic computations), IO a values represent values that have side-effects, etc.

Monads are a natural extension of applicative functors and with them we're concerned with this: if
you have a value with a context, m a, how do you apply to it a function that takes a normal a and
returns a value with a context? That is, how do you apply a function of type a -> m b to a value of
type m a? So essentially, we will want this function:

(>>=) :: (Monad m) => m a -> (a -> m b) -> m b

f we have a fancy value and a function that takes a normal value but returns a fancy value, how do
we feed that fancy value into the function? This is the main question that we will concern ourselves
when dealing with monads. We write m a instead of f a because the m stands for Monad, but monads are
just applicative functors that support >>=. The >>= function is pronounced as bind.

Now that we have a vague idea of what monads are about, let's see if we can make that idea a bit
less vague.

Much to no one's surprise, Maybe is a monad, so let's explore it a bit more and see if we can
combine it with what we know about monads.

As a Functor: fmap or <$>
When we looked at Maybe as a functor, we saw that if we want to fmap a function over it, it gets
mapped over the insides if it's a Just value, otherwise the Nothing is kept because there's nothing
to map it over!

As an Applicative: <*>
When we use <*> to apply a function inside a Maybe to a value that's inside a Maybe, they both have
to be Just values for the result to be a Just value, otherwise the result is Nothing. It makes sense
because if you're missing either the function or the thing you're applying it to, you can't make
something up out of thin air, so you have to propagate the failure.

As a Monad: >>=
>>= would take a Maybe a value and a function of type a -> Maybe b and somehow apply the function to
the Maybe a. To figure out how it does that, we can use the intuition that we have from Maybe being
an applicative functor. Let's say that we have a function \x -> Just (x+1). It takes a number, adds
1 to it and wraps it in a Just:

  ghci> (\x -> Just (x+1)) 1
  => Just 2
  ghci> (\x -> Just (x+1)) 100
  => Just 101

If we feed it 1, it evaluates to Just 2. If we give it the number 100, the result is Just 101.  Now
here's the kicker: how do we feed a Maybe value to this function? If we think about how Maybe acts
as an applicative functor, answering this is pretty easy. If we feed it a Just value, take what's
inside the Just and apply the function to it. If give it a Nothing, hmm, well, then we're left with
a function but Nothing to apply it to. In that case, let's just do what we did before and say that
the result is Nothing.

Instead of calling it >>=, let's call it applyMaybe for now. It takes a Maybe a and a function that
returns a Maybe b and manages to apply that function to the Maybe a:

  applyMaybe :: Maybe a -> (a -> Maybe b) -> Maybe b
  applyMaybe Nothing f  = Nothing
  applyMaybe (Just x) f = f x

Okay, now let's play with it for a bit. We'll use it as an infix function so that the Maybe value is
on the left side and the function on the right:

  ghci> Just 3 `applyMaybe` \x -> Just (x+1)
  => Just 4
  ghci> Just "smile" `applyMaybe` \x -> Just (x ++ " :)")
  => Just "smile :)"
  ghci> Nothing `applyMaybe` \x -> Just (x+1)
  => Nothing
  ghci> Nothing `applyMaybe` \x -> Just (x ++ " :)")
  => Nothing

If the monadic value on the left is a Nothing, the whole thing is Nothing. And if the function on
the right returns a Nothing, the result is Nothing again. This is very similar to when we used Maybe
as an applicative and we got a Nothing result if somewhere in there was a Nothing.

You might be asking yourself, how is this useful? It may seem like applicative functors are stronger
than monads, since applicative functors allow us to take a normal function and make it operate on
values with contexts. We'll see that monads can do that as well because they're an upgrade of
applicative functors, and that they can also do some cool stuff that applicative functors can't.



The Monad type class

Just like functors have the Functor type class and applicative functors have the Applicative type
class, monads come with their own type class: Monad! Wow, who would have thought? This is what the
type class looks like:

  class Monad m where
      return :: a -> m a

      (>>=) :: m a -> (a -> m b) -> m b

      (>>) :: m a -> m b -> m b
      x >> y = x >>= \_ -> y

      fail :: String -> m a
      fail msg = error msg

It says class Monad m where. But wait, didn't we say that monads are just beefed up applicative
functors? Shouldn't there be a class constraint in there along the lines of class
(Applicative m) = > Monad m where so that a type has to be an applicative functor first before it
can be made a monad? Well, there should, but when Haskell was made, it hadn't occured to people that
applicative functors are a good fit for Haskell so they weren't in there. But rest assured, every
monad is an applicative functor, even if the Monad class declaration doesn't say so.

The first function that the Monad type class defines is return. It's the same as pure, only with a
different name. Its type is (Monad m) => a -> m a. It takes a value and puts it in a minimal default
context that still holds that value. In other words, it takes something and wraps it in a monad. It
always does the same thing as the pure function from the Applicative type class, which means we're
already acquainted with return.

Just a reminder: return is nothing like the return that's in most other languages. It doesn't end
function execution or anything, it just takes a normal value and puts it in a context.

The next function is >>=, or bind. It's like function application, only instead of taking a normal
value and feeding it to a normal function, it takes a monadic value (that is, a value with a
context) and feeds it to a function that takes a normal value but returns a monadic value.

Next up, we have >>. We won't pay too much attention to it for now because it comes with a default
implementation and we pretty much never implement it when making Monad instances.

The final function of the Monad type class is fail. We never use it explicitly in our code. Instead,
it's used by Haskell to enable failure in a special syntactic construct for monads that we'll meet
later. We don't need to concern ourselves with fail too much for now.

Now that we know what the Monad type class looks like, let's take a look at how Maybe is an instance
of Monad!

  instance Monad Maybe where
      return x = Just x
      Nothing >>= f = Nothing
      Just x >>= f = f x
      fail _ = Nothing

return is the same as pure, so that one's a no-brainer. We do what we did in the Applicative type
class and wrap it in a Just.

The >>= function is the same as our applyMaybe. When feeding the Maybe a to our function, we keep in
mind the context and return a Nothing if the value on the left is Nothing because if there's no
value then there's no way to apply our function to it. If it's a Just we take what's inside and
apply f to it.

We can play around with Maybe as a monad:

  ghci> return "WHAT" :: Maybe String
  => Just "WHAT"
  ghci> Just 9 >>= \x -> return (x*10)
  => Just 90
  ghci> Nothing >>= \x -> return (x*10)
  => Nothing

Notice how when we fed Just 9 to the function \x -> return (x*10), the x took on the value 9 inside
the function. It seems as though we were able to extract the value from a Maybe without
pattern-matching. And we still didn't lose the context of our Maybe value, because when it's
Nothing, the result of using >>= will be Nothing as well.




Walk the line

Now that we know how to feed a Maybe a value to a function of type a -> Maybe b while taking into
account the context of possible failure, let's see how we can use >>= repeatedly to handle
computations of several Maybe a values.

Applicative functors don't allow for the applicative values to interact with each other very much.
They can, at best, be used as parameters to a function by using the applicative style. The
applicative operators will fetch their results and feed them to the function in a manner appropriate
for each applicative and then put the final applicative value together, but there isn't that much
interaction going on between them.

With Monads, however, each step relies on the previous one's result.

Consider the following example.

  type Birds = Int
  type Pole = (Birds,Birds)

  landLeft :: Birds -> Pole -> Maybe Pole
  landLeft n (left,right)
      | abs ((left + n) - right) < 4 = Just (left + n, right)
      | otherwise                    = Nothing

  landRight :: Birds -> Pole -> Maybe Pole
  landRight n (left,right)
      | abs (left - (right + n)) < 4 = Just (left, right + n)
      | otherwise                    = Nothing

  banana :: Pole -> Maybe Pole
  banana _ = Nothing

  ghci> return (0,0) >>= landRight 2 >>= landLeft 2 >>= landRight 2
  => Just (2,4)
  ghci> return (0,0) >>= landLeft 1 >>= landRight 4 >>= landLeft (-1) >>= landRight (-2)
  => Nothing
  ghci> return (0,0) >>= landLeft 1 >>= banana >>= landRight 1
  => Nothing
  ghci> return (0,0) >>= landLeft 1 >> Nothing >>= landRight 1
  => Nothing

Consider if we hadn't been clever enough to use =>> feature of monad:

  routine :: Maybe Pole
  routine = case landLeft 1 (0,0) of
      Nothing -> Nothing
      Just pole1 -> case landRight 4 pole1 of
          Nothing -> Nothing
          Just pole2 -> case landLeft 2 pole2 of
              Nothing -> Nothing
              Just pole3 -> landLeft 1 pole3


In this section, we took some functions that we had and saw that they would work better if the
values that they returned supported failure. By turning those values into Maybe values and replacing
normal function application with >>=, we got a mechanism for handling failure pretty much for free,
because >>= is supposed to preserve the context of the value to which it applies functions. In this
case, the context was that our values were values with failure and so when we applied functions to
such values, the possibility of failure was always taken into account.



do notation

Monads in Haskell are so useful that they got their own special syntax called do notation. We've
already encountered do notation when we were doing I/O and there we said that it was for gluing
together several I/O actions into one. Well, as it turns out, do notation isn't just for IO, but can
be used for any monad. Its principle is still the same: gluing together monadic values in sequence.
We're going to take a look at how do notation works and why it's useful.

Consider this familiar example of monadic application:

  ghci> Just 3 >>= (\x -> Just (show x ++ "!"))
  => Just "3!"

Been there, done that. Feeding a monadic value to a function that returns one, no big deal. Notice
how when we do this, x becomes 3 inside the lambda. Once we're inside that lambda, it's just a
normal value rather than a monadic value. Now, what if we had another >>= inside that function?
Check this out:

  ghci> Just 3 >>= (\x -> Just "!" >>= (\y -> Just (show x ++ y)))
  => Just "3!"

Ah, a nested use of >>=! In the outermost lambda, we feed Just "!" to the
lambda \y -> Just (show x ++ y). Inside this lambda, the y becomes "!". x is still 3 because we got
it from the outer lambda. All this sort of reminds me of the following expression:

  ghci> let x = 3; y = "!" in show x ++ y
  => "3!"

The main difference between these two is that the values in the former example are monadic. They're
values with a failure context. We can replace any of them with a failure:

  ghci> Nothing >>= (\x -> Just "!" >>= (\y -> Just (show x ++ y)))
  => Nothing
  ghci> Just 3 >>= (\x -> Nothing >>= (\y -> Just (show x ++ y)))
  => Nothing
  ghci> Just 3 >>= (\x -> Just "!" >>= (\y -> Nothing))
  => Nothing

To further illustrate this point, let's write this in a script and have each Maybe value take up its
own line:

  foo :: Maybe String
  foo = Just 3   >>= (\x ->
        Just "!" >>= (\y ->
        Just (show x ++ y)))

To save us from writing all these annoying lambdas, Haskell gives us do notation. It allows us to
write the previous piece of code like this:

  foo :: Maybe String
  foo = do
      x <- Just 3
      y <- Just "!"
      Just (show x ++ y)

It would seem as though we've gained the ability to temporarily extract things from Maybe values
without having to check if the Maybe values are Just values or Nothing values at every step. How
cool! If any of the values that we try to extract from are Nothing, the whole do expression will
result in a Nothing. We're yanking out their (possibly existing) values and letting >>= worry about
the context that comes with those values. It's important to remember that do expressions are just
different syntax for chaining monadic values.

In a do expression, every line is a monadic value. To inspect its result, we use <-. If we have a
Maybe String and we bind it with <- to a variable, that variable will be a String, just like when we
used >>= to feed monadic values to lambdas. The last monadic value in a do expression, like
Just (show x ++ y) here, can't be used with <- to bind its result, because that wouldn't make sense
if we translated the do expression back to a chain of >>= applications. Rather, its result is the
result of the whole glued up monadic value, taking into account the possible failure of any of the
previous ones.

For instance, examine the following line:

  ghci> Just 9 >>= (\x -> Just (x > 8))
  => Just True

Because the left parameter of >>= is a Just value, the lambda is applied to 9 and the result is a
Just True. If we rewrite this in do notation, we get:
  marySue :: Maybe Bool
  marySue = do
      x <- Just 9
      Just (x > 8)

If we compare these two, it's easy to see why the result of the whole monadic value is the result of
the last monadic value in the do expression with all the previous ones chained into it.

Because do expressions are written line by line, they may look like imperative code to some people.
But the thing is, they're just sequential, as each value in each line relies on the result of the
previous ones, along with their contexts (in this case, whether they succeeded or failed).

Look how our above implementation is transformed;

Before:
  routine :: Maybe Pole
  routine = return (0,0) >>= landLeft 1 >> Nothing >>= landRight 1

After:
  routine :: Maybe Pole
  routine = do
    start <- return (0,0)
    first <- landLeft 2 start
    Nothing
    second <- landRight 2 first
    landLeft 1 second

When to use do notation and when to explicitly use >>= is up to you. I think this example lends
itself to explicitly writing >>= because each step relies specifically on the result of the previous
one. With do notation, we had to specifically write on which pole the birds are landing, but every
time we used that came directly before. But still, it gave us some insight into do notation.



The list monad

So far, we've seen how Maybe values can be viewed as values with a failure context and how we can
incorporate failure handling into our code by using >>= to feed them to functions. In this section,
we're going to take a look at how to use the monadic aspects of lists to bring non-determinism into
our code in a clear and readable manner.

We've already talked about how lists represent non-deterministic values when they're used as
applicatives. A value like 5 is deterministic. It has only one result and we know exactly what it
is. On the other hand, a value like [3,8,9] contains several results, so we can view it as one
value that is actually many values at the same time. Using lists as applicative functors showcases
this non-determinism nicely:

  ghci> (*) <$> [1,2,3] <*> [10,100,1000]
  [10,100,1000,20,200,2000,30,300,3000]

All the possible combinations of multiplying elements from the left list with elements from the
right list are included in the resulting list. When dealing with non-determinism, there are many
choices that we can make, so we just try all of them, and so the result is a non-deterministic
value as well, only it has many more results.

This context of non-determinism translates to monads very nicely. Let's go ahead and see what the
Monad instance for lists looks like:

  instance Monad [] where
      return x = [x]
      xs >>= f = concat (map f xs)
      fail _ = []

return does the same thing as pure, so we should already be familiar with return for lists. It takes
a value and puts it in a minimal default context that still yields that value. In other words, it
makes a list that has only that one value as its result. This is useful for when we want to just
wrap a normal value into a list so that it can interact with non-deterministic values.

To understand how >>= works for lists, it's best if we take a look at it in action to gain some
intuition first. >>= is about taking a value with a context (a monadic value) and feeding it to a
function that takes a normal value and returns one that has context. If that function just produced
a normal value instead of one with a context, >>= wouldn't be so useful because after one use, the
context would be lost. Anyway, let's try feeding a non-deterministic value to a function:

  ghci> [3,4,5] >>= \x -> [x,-x]
  [3,-3,4,-4,5,-5]

When we used >>= with Maybe, the monadic value was fed into the function while taking care of
possible failures. Here, it takes care of non-determinism for us. [3,4,5] is a non-deterministic
value and we feed it into a function that returns a non-deterministic value as well. The result is
also non-deterministic, and it features all the possible results of taking elements from the list
[3,4,5] and passing them to the function \x -> [x,-x]. This function takes a number and produces
two results: one negated and one that's unchanged. So when we use >>= to feed this list to the
function, every number is negated and also kept unchanged. The x from the lambda takes on every
value from the list that's fed to it.

To see how this is achieved, we can just follow the implementation. First, we start off with the
list [3,4,5]. Then, we map the lambda over it and the result is the following:

  [[3,-3],[4,-4],[5,-5]]

The lambda is applied to every element and we get a list of lists. Finally, we just flatten the list
and voila! We've applied a non-deterministic function to a non-deterministic value!

Non-determinism also includes support for failure. The empty list [] is pretty much the equivalent
of Nothing, because it signifies the absence of a result. That's why failing is just defined as the
empty list. The error message gets thrown away. Let's play around with lists that fail:

  ghci> [] >>= \x -> ["bad","mad","rad"]
  => []
  ghci> [1,2,3] >>= \x -> []
  => []

In the first line, an empty list is fed into the lambda. Because the list has no elements, none of
them can be passed to the function and so the result is an empty list. This is similar to feeding
Nothing to a function. In the second line, each element gets passed to the function, but the element
is ignored and the function just returns an empty list. Because the function fails for every element
that goes in it, the result is a failure.


Just like with Maybe values, we can chain several lists with >>=, propagating the non-determinism:

  ghci> [1,2] >>= \n -> ['a','b'] >>= \ch -> return (n,ch)
  => [(1,'a'),(1,'b'),(2,'a'),(2,'b')]

When you have non-deterministic values interacting, you can view their computation as a tree where
every possible result in a list represents a separate branch.




Monad laws

Just like applicative functors, and functors before them, monads come with a few laws that all monad
instances must abide by. Just because something is made an instance of the Monad type class doesn't
mean that it's a monad, it just means that it was made an instance of a type class. For a type to
truly be a monad, the monad laws must hold for that type. These laws allow us to make reasonable
assumptions about the type and its behavior.

Haskell allows any type to be an instance of any type class as long as the types check out. It can't
check if the monad laws hold for a type though, so if we're making a new instance of the Monad type
class, we have to be reasonably sure that all is well with the monad laws for that type. We can rely
on the types that come with the standard library to satisfy the laws, but later when we go about
making our own monads, we're going to have to manually check the if the laws hold. But don't worry,
they're not complicated.

Left identity

The first monad law states that if we take a value, put it in a default context with return and then
feed it to a function by using >>=, it's the same as just taking the value and applying the function
to it. To put it formally:

  return x >>= f is the same damn thing as f x

If you look at monadic values as values with a context and return as taking a value and putting it
in a default minimal context that still presents that value as its result, it makes sense, because
if that context is really minimal, feeding this monadic value to a function shouldn't be much
different than just applying the function to the normal value, and indeed it isn't different at all.

Right identity

The second law states that if we have a monadic value and we use >>= to feed it to return, the
result is our original monadic value. Formally:

  m >>= return is no different than just m

This one might be a bit less obvious than the first one, but let's take a look at why it should
hold. When we feed monadic values to functions by using >>=, those functions take normal values and
return monadic ones. return is also one such function, if you consider its type. Like we said,
return puts a value in a minimal context that still presents that value as its result. This means
that, for instance, for Maybe, it doesn't introduce any failure and for lists, it doesn't introduce
any extra non-determinism. Here's a test run for a few monads:


  ghci> Just "move on up" >>= (\x -> return x)
  => Just "move on up"
  ghci> [1,2,3,4] >>= (\x -> return x)
  => [1,2,3,4]
  ghci> putStrLn "Wah!" >>= (\x -> return x)
  => Wah!

Left identity and right identity are basically laws that describe how return should behave. It's an
important function for making normal values into monadic ones and it wouldn't be good if the monadic
value that it produced did a lot of other stuff.


Associativity

The final monad law says that when we have a chain of monadic function applications with >>=, it
shouldn't matter how they're nested. Formally written:

  Doing (m >>= f) >>= g is just like doing m >>= (\x -> f x >>= g)

Hmmm, now what's going on here? We have one monadic value, m and two monadic functions f and g. When
we're doing (m >>= f) >>= g, we're feeding m to f, which results in a monadic value. Then, we feed
that monadic value to g.

In the expression m >>= (\x -> f x >>= g), we take a monadic value and we feed it to a function that
feeds the result of f x to g. It's not easy to see how those two are equal, so let's take a look at
an example that makes this equality a bit clearer.

To simulate birds landing on his balancing pole, we made a chain of several functions that might
produce failure:

  ghci> return (0,0) >>= landRight 2 >>= landLeft 2 >>= landRight 2
  => Just (2,4)

We started with Just (0,0) and then bound that value to the next monadic function, landRight 2. The
result of that was another monadic value which got bound into the next monadic function, and so on.
If we were to explicitly parenthesize this, we'd write:

  ghci> ((return (0,0) >>= landRight 2) >>= landLeft 2) >>= landRight 2
  => Just (2,4)


This is very similar to how if f is a normal function, (f . g) . h is the same as f . (g . h),
f . id is always the same as f and id . f is also just f.

